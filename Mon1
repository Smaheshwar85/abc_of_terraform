
Introduction
The Common Data Platform (CDP) serves as the centralized hub for managing all transactional data within the organization. This platform is designed to streamline data storage, integration, and accessibility, acting as the single source of truth for organizational data. By consolidating data into a unified system, the CDP ensures scalability, governance, and seamless data access across various domains.

The platform is built to provide robust interfaces, enabling data to be accessed and utilized by downstream systems through APIs and other integration mechanisms. This ensures that the data is not only stored efficiently but also readily available for analytics, reporting, and operational processes. The CDP represents a critical step in establishing a comprehensive data infrastructure that supports innovation, compliance, and efficient data-driven decision-making across the organization.

Scope
The scope of the Common Data Platform includes the following key components:

Centralized Data Management:

All transactional data and relevant reference/master data will be stored within the CDP.
The platform will serve as a unified repository, eliminating data silos and avoiding redundant data copies.
Data will be stored and made available in various forms, such as:
"As received from source" for raw data needs and traceability.
"Integrated with reference/master data" to ensure consistency and integrity.
"Enriched for specific purposes" to meet business-specific requirements.
Data Accessibility and Interfaces:

Data will be made accessible through well-defined APIs, enabling seamless integration with other systems and tools.
Reporting and analytics solutions, including tools like Power BI, will consume data directly from the CDP.
Data products will be created and governed through defined SLAs and ownership agreements.
Data Governance and Compliance:

The CDP will enforce data governance rules to ensure compliance with regulatory and organizational standards.
A strict separation will be maintained between data integration processes and data provisioning for analytical or operational purposes.
Clear ownership of data pipelines and processes will be defined to enhance accountability and transparency.
Scalability and Efficiency:

The platform is designed to scale with the organization’s needs, supporting high data volumes and complex use cases.
It enables faster time-to-market for business features by streamlining deployment and integration processes.
By acting as a single source of truth, the CDP minimizes redundancies and ensures efficient data processing.
Decentralized Responsibility:

While the CDP acts as the central platform, responsibility for data ownership, pipelines, and processes will remain decentralized across business units and domains.
A central data catalog will provide visibility into data assets, ensuring transparency and ease of access.
The Common Data Platform is a foundational element of the organization’s data strategy, enabling seamless data integration, governance, and access. It empowers the organization to achieve operational excellence, meet compliance requirements, and drive innovation through reliable and efficient data management.







You said:
OPlease add the above context  and provide the security concept like  "this security document  or the scope of the document is highlight the different intergraion point and ad different security measure for data "please add the context in deatils

2/2

ChatGPT said:
ChatGPT
Introduction
The Common Data Platform (CDP) serves as a central data integration hub for the organization, consolidating all transactional data into a unified repository. It provides a single source of truth for storing, managing, and accessing data through well-defined interfaces such as APIs. The CDP is designed to facilitate seamless data sharing while ensuring compliance, scalability, and efficient data governance across business units and operational domains.

The platform enables data to be consumed directly from the CDP for analytics, reporting, and operational purposes, supporting diverse use cases and business needs. By enforcing data governance and establishing clear ownership, the CDP aims to eliminate data silos, reduce redundancy, and provide secure, consistent, and reliable access to data assets.

This document specifically focuses on the security aspects of the CDP, addressing the critical integration points and outlining the necessary security measures to ensure the confidentiality, integrity, and availability of data throughout its lifecycle.

Scope
The scope of this security document is to highlight the integration points of the Common Data Platform and define the security measures required to safeguard data at each stage of its processing, storage, and consumption. This includes the following key areas:

Integration Points:
Data Ingestion:

Securely ingest transactional and reference/master data into the CDP from various internal and external data sources.
Ensure data validation, encryption, and access control during ingestion to prevent unauthorized access or tampering.
Data Storage:

Store all data in the CDP with encryption at rest using industry-standard algorithms.
Implement access control policies to ensure that only authorized users and systems can access sensitive data.
Maintain detailed audit logs for data access and modifications.
Data Access via APIs:

Provide secure API endpoints for accessing data stored in the CDP.
Enforce authentication and authorization mechanisms to control access to APIs.
Use encrypted communication protocols (e.g., HTTPS/TLS) to protect data in transit.
Data Consumption for Reporting and Analytics:

Establish role-based access controls (RBAC) for analytics tools and consumers.
Ensure that data products created from the CDP are governed by strict SLAs and data usage agreements.
Data Governance and Ownership:

Define clear ownership and accountability for all data pipelines, processes, and assets stored within the CDP.
Maintain a centralized data catalog to provide visibility into data assets and their associated security attributes.
Security Measures
To secure the Common Data Platform and protect sensitive data, the following measures will be implemented:

Authentication and Authorization:

Integrate with enterprise-grade identity and access management (IAM) systems to authenticate users and systems accessing the CDP.
Enforce least privilege principles by granting users and services only the permissions necessary for their roles.
Encryption:

Encrypt all data at rest using robust encryption standards such as AES-256.
Secure data in transit using HTTPS/TLS to prevent eavesdropping and man-in-the-middle attacks.
Monitoring and Logging:

Implement logging and monitoring solutions to track all data access, modifications, and API usage.
Use anomaly detection systems to identify and mitigate potential security threats in real time.
Data Classification and Masking:

Classify data based on sensitivity levels and apply appropriate security controls.
Use data masking techniques for sensitive information to minimize exposure during processing and analytics.
Regular Security Audits:

Conduct regular security audits to ensure compliance with organizational and regulatory standards.
Perform penetration testing to identify and address vulnerabilities in the platform.
Incident Response:

Establish an incident response plan to handle security breaches effectively.
Include procedures for data recovery, forensic analysis, and root cause identification.
Data Governance Policies:

Enforce computational governance to automatically ensure compliance with organizational data policies.
Maintain a strict separation between data integration processes and data accessibility to minimize risks.
Conclusion
The Common Data Platform is a critical asset for the organization, providing a scalable and secure foundation for data integration and analytics. This security document highlights the various integration points within the platform and the measures required to safeguard data at every stage. By implementing these measures, the organization ensures the confidentiality, integrity, and availability of its data, fostering trust and enabling data-driven decision-making across all business units.






You said:
write the scope of the security doocument and paragraph format
ChatGPT said:
ChatGPT
Scope of the Security Document
This security document outlines the critical integration points of the Common Data Platform (CDP) and establishes the security measures necessary to ensure the confidentiality, integrity, and availability of data throughout its lifecycle. The CDP serves as a centralized data integration hub for the organization, facilitating the ingestion, storage, and consumption of transactional and reference/master data. This document focuses on securing each stage of data processing, from ingestion to consumption, and highlights the protocols required to safeguard data as it flows through APIs, data pipelines, and analytics systems.

The scope includes securing data ingestion processes to ensure that all data entering the platform is validated and protected against unauthorized access or tampering. It also covers encryption and access control mechanisms to secure data stored within the CDP, ensuring only authorized users and systems can access sensitive information. Additionally, the document addresses the protection of data during transmission via APIs, enforcing robust authentication, authorization, and encryption protocols. For data consumers, such as reporting and analytics tools, this document emphasizes the implementation of role-based access control (RBAC) and strict service-level agreements (SLAs) to govern data usage.

Furthermore, the document defines the responsibilities of data owners and consumers, detailing governance practices to maintain accountability and transparency across all data pipelines. It also includes strategies for monitoring, logging, and auditing to detect and mitigate security threats proactively. By focusing on these aspects, the document aims to provide a comprehensive security framework that aligns with organizational standards, regulatory requirements, and industry best practices.







=================================================================================================================================================================



Introduction
The Common Data Platform (CDP) serves as the centralized hub for managing all transactional data within the organization. This platform is designed to streamline data storage, integration, and accessibility, acting as the single source of truth for organizational data. By consolidating data into a unified system, the CDP ensures scalability, governance, and seamless data access across various domains.

The platform is built to provide robust interfaces, enabling data to be accessed and utilized by downstream systems through APIs and other integration mechanisms. This ensures that the data is not only stored efficiently but also readily available for analytics, reporting, and operational processes. The CDP represents a critical step in establishing a comprehensive data infrastructure that supports innovation, compliance, and efficient data-driven decision-making across the organization.

Scope
The scope of the Common Data Platform includes the following key components:

Centralized Data Management:

All transactional data and relevant reference/master data will be stored within the CDP.
The platform will serve as a unified repository, eliminating data silos and avoiding redundant data copies.
Data will be stored and made available in various forms, such as:
"As received from source" for raw data needs and traceability.
"Integrated with reference/master data" to ensure consistency and integrity.
"Enriched for specific purposes" to meet business-specific requirements.
Data Accessibility and Interfaces:

Data will be made accessible through well-defined APIs, enabling seamless integration with other systems and tools.
Reporting and analytics solutions, including tools like Power BI, will consume data directly from the CDP.
Data products will be created and governed through defined SLAs and ownership agreements.
Data Governance and Compliance:

The CDP will enforce data governance rules to ensure compliance with regulatory and organizational standards.
A strict separation will be maintained between data integration processes and data provisioning for analytical or operational purposes.
Clear ownership of data pipelines and processes will be defined to enhance accountability and transparency.
Scalability and Efficiency:

The platform is designed to scale with the organization’s needs, supporting high data volumes and complex use cases.
It enables faster time-to-market for business features by streamlining deployment and integration processes.
By acting as a single source of truth, the CDP minimizes redundancies and ensures efficient data processing.
Decentralized Responsibility:

While the CDP acts as the central platform, responsibility for data ownership, pipelines, and processes will remain decentralized across business units and domains.
A central data catalog will provide visibility into data assets, ensuring transparency and ease of access.
The Common Data Platform is a foundational element of the organization’s data strategy, enabling seamless data integration, governance, and access. It empowers the organization to achieve operational excellence, meet compliance requirements, and drive innovation through reliable and efficient data management.


====================================================================================================================================================
# Assert that Workload Identity User IAM role is assigned to the Kubernetes service account at the service account level
  assert {
    condition     = google_service_account_iam_member.metric_reader_viewer.role == "roles/iam.workloadIdentityUser"
    error_message = "Workload Identity User role not correctly assigned."
  }

  # Assert that the `member` for Workload Identity User includes the correct Kubernetes service account
  assert {
    condition     = google_service_account_iam_member.metric_reader_viewer.member == "serviceAccount:jproject234.bc.id.googlecloud[grafana-monitoring/metric_reader]"
    error_message = "Workload Identity User member string is incorrect."
  }


hcl
Copy code
resource "google_service_account" "metric_reader" {
  account_id   = var.account_id
  project_id   = var.project_id
  display_name = var.display_name
}

resource "google_project_iam_member" "metric_reader_viewer" {
  project_id = var.project_id
  role       = "roles/monitoring.viewer"
  member     = "serviceAccount:${google_service_account.metric_reader.email}"
}

resource "google_project_iam_member" "workload_identity_user" {
  project_id      = var.project_id
  role            = "roles/iam.workloadIdentityUser"
  member          = "serviceAccount:${var.cluster_project_id}.svc.id.goog[${var.k8s_namespace}/${var.metric_reader_k8s_account}]"
}

# Output service account email
output "service_account_email" {
  value = google_service_account.metric_reader.email
}

output "service_account_name" {
  value = google_service_account.metric_reader.name
}
Variables: service_account_module/variables.tf
hcl
Copy code
variable "account_id" {
  description = "The account id for the service account"
  type        = string
}

variable "project_id" {
  description = "The GCP project id"
  type        = string
}

variable "display_name" {
  description = "The display name for the service account"
  type        = string
  default     = "Grafana Service Account"
}

variable "cluster_project_id" {
  description = "The cluster project id where the workload identity will be used"
  type        = string
}

variable "k8s_namespace" {
  description = "The Kubernetes namespace where the service account resides"
  type        = string
}

variable "metric_reader_k8s_account" {
  description = "The name of the Kubernetes service account"
  type        = string
}
Example Usage for an Application: application/main.tf
hcl
Copy code
module "grafana_service_account" {
  source = "../service_account_module"

  account_id                = "grafana-metric-reader"
  project_id                = "abc123"
  display_name              = "Grafana Metric Reader Service Account"
  cluster_project_id        = "jproject234"
  k8s_namespace             = "grafana-monitoring"
  metric_reader_k8s_account = "metric_reader"
}
Test Configuration: basic.tftest.hcl
hcl
Copy code
module "test_grafana_service_account" {
  source = "./service_account_module"

  account_id                = "test-metric-reader"
  project_id                = "test-project-id"
  display_name              = "Test Metric Reader"
  cluster_project_id        = "test-cluster-project-id"
  k8s_namespace             = "test-namespace"
  metric_reader_k8s_account = "test-metric-reader"
}

output "service_account_email" {
  value = module.test_grafana_service_account.service_account_email
}

output "service_account_name" {
  value = module.test_grafana_service_account.service_account_name
}
How to Test
To run the test, execute the following command in the folder where basic.tftest.hcl resides:

bash
Copy code
terraform init
terraform apply
This setup ensures that the module is reusable and flexible for different applications, and the tftest.hcl allows for basic testing to validate that the service account and IAM bindings are created correctly.






You said:
create a test file using run "basic_apply_valid_variables"
ChatGPT said:
ChatGPT
To create a test file using terraform's tftest framework and running basic_apply_valid_variables, here’s how you can structure the test file to verify that the module is working correctly. The test will ensure that valid inputs lead to a successful creation of the resources.

Test File: test_service_account.tf
hcl
Copy code
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
  }
}

# Define the test function for applying the valid variables
tftest "basic_apply_valid_variables" {
  test_name = "Basic apply with valid variables"
  
  # Load variables from a valid configuration
  variables = {
    account_id                = "test-metric-reader"
    project_id                = "test-project-id"
    display_name              = "Test Metric Reader"
    cluster_project_id        = "test-cluster-project-id"
    k8s_namespace             = "test-namespace"
    metric_reader_k8s_account = "test-metric-reader"
  }

  # Expected outputs
  expected_outputs = {
    service_account_email = "test-metric-reader@test-project-id.iam.gserviceaccount.com"
    service_account_name  = "projects/test-project-id/serviceAccounts/test-metric-reader@test-project-id.iam.gserviceaccount.com"
  }

  # Execute basic apply
  apply = true
}
Explanation:
Variables: The variables block in the tftest uses the expected values as input for your module. You can replace these with other valid data if needed for further testing.
Expected Outputs: These values are what you expect the module to output after applying the basic_apply_valid_variables test. Make sure they are accurate to your setup (e.g., the project ID and account ID).
Apply: The test will apply the configuration using the provided variables.
Running the Test
You can run this test by executing the following commands:

Initialize Terraform:

bash
Copy code
terraform init
Run the Test:

bash
Copy code
terraform apply -var-file=test_service_account.tf
This test will verify that when valid variables are supplied, the module creates the service account and applies the required IAM roles successfully.






You said:
add the different assert condition with the above test code
ChatGPT said:
ChatGPT
To enhance the test with different assert conditions, you can add checks for specific outputs, resource existence, and state validation using tftest. Here’s how you can expand the test by asserting the expected behavior of the Terraform module.

Test File with Asserts: test_service_account.tf
hcl
Copy code
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
  }
}

# Define the test function for applying the valid variables
tftest "basic_apply_valid_variables" {
  test_name = "Basic apply with valid variables"
  
  # Load variables from a valid configuration
  variables = {
    account_id                = "test-metric-reader"
    project_id                = "test-project-id"
    display_name              = "Test Metric Reader"
    cluster_project_id        = "test-cluster-project-id"
    k8s_namespace             = "test-namespace"
    metric_reader_k8s_account = "test-metric-reader"
  }

  # Execute basic apply
  apply = true

  # Expected outputs
  expected_outputs = {
    service_account_email = "test-metric-reader@test-project-id.iam.gserviceaccount.com"
    service_account_name  = "projects/test-project-id/serviceAccounts/test-metric-reader@test-project-id.iam.gserviceaccount.com"
  }

  # Assert conditions

  # Assert that the service account email matches the expected format
  assert {
    condition     = module.test_grafana_service_account.service_account_email == "test-metric-reader@test-project-id.iam.gserviceaccount.com"
    error_message = "Service account email does not match the expected format."
  }

  # Assert that the service account name is in the correct project
  assert {
    condition     = module.test_grafana_service_account.service_account_name == "projects/test-project-id/serviceAccounts/test-metric-reader@test-project-id.iam.gserviceaccount.com"
    error_message = "Service account name is incorrect or in the wrong project."
  }

  # Assert that the Google IAM role "monitoring.viewer" is applied to the service account
  assert {
    condition     = google_project_iam_member.metric_reader_viewer.role == "roles/monitoring.viewer"
    error_message = "Monitoring viewer role not assigned to the service account."
  }

  # Assert that Workload Identity User IAM role is correctly assigned to the Kubernetes service account
  assert {
    condition     = google_project_iam_member.workload_identity_user.role == "roles/iam.workloadIdentityUser"
    error_message = "Workload Identity User role not correctly assigned."
  }

  # Assert the service account resource exists in the state
  assert {
    condition     = google_service_account.metric_reader != null
    error_message = "Service account resource not created."
  }

  # Assert that the service account display name matches the expected value
  assert {
    condition     = google_service_account.metric_reader.display_name == "Test Metric Reader"
    error_message = "Service account display name does not match."
  }

}
Explanation of Assertions:
Service Account Email Assertion: This checks if the service_account_email output matches the expected format.

Service Account Name Assertion: Ensures that the service account is created in the correct project and has the right naming convention.

IAM Role "monitoring.viewer" Assertion: Verifies that the roles/monitoring.viewer IAM role has been assigned to the service account.

Workload Identity User IAM Role Assertion: Checks whether the Kubernetes service account is correctly bound to the roles/iam.workloadIdentityUser role.

Resource Existence Assertion: Confirms that the service account resource was created and exists in the Terraform state.

Service Account Display Name Assertion: Ensures that the display name of the created service account matches the provided value.

Running the Test with Asserts
Initialize Terraform:

bash
Copy code
terraform init
Run the Test:

bash
Copy code
terraform apply -var-file=test_service_account.tf
The test will now check these conditions, ensuring the service account, IAM roles, and related resources behave as expected. If any assertion fails, an error message will indicate the cause.






===================================================================================================================================================================




import os
import sys
import time
import logging
from google.api_core.client_options import ClientOptions
from google.cloud.monitoring import MetricServiceClient
from google.cloud import logging as gcp_logging  # Import Google Cloud Logging
from opentelemetry import metrics
from opentelemetry.exporter.cloud_monitoring import CloudMonitoringMetricsExporter
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import (
    get_aggregated_resources,
    ProcessResourceDetector
)
from google.oauth2 import service_account  # Import service account for authentication

# Fetch project ID and service account file for Monitoring API
GOOGLE_CLOUD_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT", "your-project-id")
SA_FILENAME = os.getenv("SA_FILENAME", "path/to/service-account-file.json")

# Define Monitoring API endpoint
MONITORING_API_ENDPOINT = os.getenv("MONITORING_API_ENDPOINT", "https://monitoring.googleapis.com")
LOGGING_API_ENDPOINT = os.getenv("LOGGING_API_ENDPOINT", "https://logging.googleapis.com")

# Function to create and return Metrics and Logging clients using client options
def create_clients(api_endpoint, logging_endpoint, sa_filename):
    print("Creating clients with API endpoint: {} and Logging endpoint: {}".format(api_endpoint, logging_endpoint))
    
    # Use service account credentials
    credentials = service_account.Credentials.from_service_account_file(sa_filename)

    # Set client options with custom API endpoints
    metrics_options = ClientOptions(credentials=credentials, api_endpoint=api_endpoint)
    logging_options = ClientOptions(credentials=credentials, api_endpoint=logging_endpoint)
    
    # Create MetricServiceClient
    print("Initializing MetricServiceClient...")
    metrics_client = MetricServiceClient(client_options=metrics_options)
    print("MetricServiceClient created successfully.")

    # Create Google Cloud Logging client with custom options
    print("Initializing Google Cloud Logging client...")
    logging_client = gcp_logging.Client(credentials=credentials)
    print("Google Cloud Logging client created successfully.")

    return metrics_client, logging_client

# Function to create and configure OpenTelemetry's Meter with Cloud Monitoring Metrics Exporter
def get_meter(api_endpoint=MONITORING_API_ENDPOINT, logging_endpoint=LOGGING_API_ENDPOINT, project_id=GOOGLE_CLOUD_PROJECT, sa_filename=SA_FILENAME):
    print("Getting meter and logging client...")
    
    # Initialize MetricServiceClient and Logging client
    metrics_client, logging_client = create_clients(api_endpoint, logging_endpoint, sa_filename)
    
    print("Setting up MeterProvider with Cloud Monitoring exporter...")
    
    # Set up the meter provider with Cloud Monitoring exporter and resource detectors
    metrics.set_meter_provider(
        MeterProvider(
            metric_readers=[
                PeriodicExportingMetricReader(
                    CloudMonitoringMetricsExporter(
                        project_id=project_id,
                        client=metrics_client  # Pass the monitoring client to the exporter
                    ),
                    export_interval_millis=5000,  # Export interval set to 5 seconds
                )
            ],
            resource=get_aggregated_resources(
                [ProcessResourceDetector(raise_on_error=True)]  # Add process resources
            ),
        )
    )
    print("MeterProvider setup completed.")
    
    meter = metrics.get_meter(__name__)  # Return the configured meter
    print("Meter created successfully.")
    
    return meter, logging_client

# Function to get additional metrics properties (can be extended as needed)
def get_metrics_properties():
    environment = os.getenv("MONITORING_ENV", "staging")
    print("Metrics properties - environment: {}".format(environment))
    
    return {
        "environment": environment,
    }

# Function to create OpenTelemetry instruments (counter for IAM errors)
def get_instruments():
    print("Getting instruments (IAM error counter)...")
    
    meter, logging_client = get_meter()
    
    # Create IAM error counter with dynamic name and description
    counter_name = os.getenv("MONITORING_METRICS_IAM_ERROR_COUNTER_NAME", "iam_error_counter")
    counter_description = os.getenv("MONITORING_METRICS_IAM_ERROR_COUNTER_DESCRIPTION", "Tracks IAM errors")
    print("Creating IAM error counter: {}, description: {}".format(counter_name, counter_description))
    
    iam_error_counter = meter.create_counter(
        name=counter_name,
        description=counter_description,
    )
    
    print("IAM error counter created successfully.")
    
    return iam_error_counter, logging_client

# Function to check for IAM errors in logs and increment the counter
def check_for_iam_errors(iam_error_counter, logging_client):
    logger = logging.getLogger(__name__)
    logger.addHandler(gcp_logging.handlers.CloudLoggingHandler(logging_client))
    logger.info("Checking for IAM errors...")
    print("Checking for IAM errors in logs...")
    
    # Create a filter to fetch logs related to IAM errors for any resource
    filter_str = 'severity="ERROR" AND textPayload:"IAM"'
    print("Log filter used: {}".format(filter_str))
    
    # Fetch logs with the filter for IAM errors
    for entry in logging_client.logger("IAM_Error_Logger").list_entries(filter_=filter_str):
        print("IAM error detected in log: {}".format(entry.payload))
        
        # Increment the IAM error counter if an error entry is found
        labels = get_metrics_properties()  # Add custom labels
        iam_error_counter.add(1, labels)
        logger.info("IAM error detected: {}".format(entry.payload))

# Main loop to check for IAM errors and update metrics periodically
def main():
    print("Starting the main loop...")
    
    # Get the IAM error counter instrument and logging client
    iam_error_counter, logging_client = get_instruments()
    
    # Loop to check for IAM errors every 5 seconds
    while True:
        print("Running IAM error check cycle...")
        check_for_iam_errors(iam_error_counter, logging_client)
        time.sleep(5)
        print("Sleeping for 5 seconds before the next check...")

if __name__ == "__main__":
    main()
kkkkkkkkkk

import os
import time
import logging
from google.api_core.client_options import ClientOptions
from google.cloud.monitoring_v3 import MetricServiceClient
from google.cloud import logging as gcp_logging  # Import Google Cloud Logging
from google.oauth2 import service_account
from opentelemetry import metrics
from opentelemetry.exporter.cloud_monitoring import CloudMonitoringMetricsExporter
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import (
    get_aggregated_resources,
    ProcessResourceDetector
)

# Fetch project ID and service account file for Monitoring API
GOOGLE_CLOUD_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT", "your-project-id")
SA_FILENAME = os.getenv("SA_FILENAME", "path/to/service-account-file.json")

# Define Monitoring API endpoint
MONITORING_API_ENDPOINT = os.getenv("MONITORING_API_ENDPOINT", "https://monitoring.googleapis.com")
LOGGING_API_ENDPOINT = os.getenv("LOGGING_API_ENDPOINT", "https://logging.googleapis.com")

# Scopes for Google Cloud Logging and Monitoring
SCOPES = [
    'https://www.googleapis.com/auth/logging.read',
    'https://www.googleapis.com/auth/monitoring'
]

# Function to create and return Metrics and Logging clients using service account credentials
def create_clients(api_endpoint: str, logging_endpoint: str, sa_filename: str) -> (MetricServiceClient, gcp_logging.Client):
    print(f"Creating clients with API endpoint: {api_endpoint} and Logging endpoint: {logging_endpoint}")
    
    # Load credentials from service account file
    credentials = service_account.Credentials.from_service_account_file(sa_filename, scopes=SCOPES)
    
    # Set client options with the API endpoint
    metrics_options = ClientOptions(api_endpoint=api_endpoint)
    logging_options = ClientOptions(api_endpoint=logging_endpoint)
    
    # Create MetricServiceClient
    print("Initializing MetricServiceClient...")
    metrics_client = MetricServiceClient(credentials=credentials, client_options=metrics_options)
    print("MetricServiceClient created successfully.")

    # Create Google Cloud Logging client
    print("Initializing Google Cloud Logging client...")
    logging_client = gcp_logging.Client(credentials=credentials, client_options=logging_options, project=GOOGLE_CLOUD_PROJECT)
    print("Google Cloud Logging client created successfully.")

    return metrics_client, logging_client

# Function to create and configure OpenTelemetry's Meter with Cloud Monitoring Metrics Exporter
def get_meter(
    api_endpoint: str = MONITORING_API_ENDPOINT,
    logging_endpoint: str = LOGGING_API_ENDPOINT,
    project_id: str = GOOGLE_CLOUD_PROJECT,
    sa_filename: str = SA_FILENAME,
) -> (metrics.Meter, gcp_logging.Client):
    print("Getting meter and logging client...")
    
    # Initialize MetricServiceClient and Logging client
    metrics_client, logging_client = create_clients(api_endpoint, logging_endpoint, sa_filename)
    
    print("Setting up MeterProvider with Cloud Monitoring exporter...")
    
    # Set up the meter provider with Cloud Monitoring exporter and resource detectors
    metrics.set_meter_provider(
        MeterProvider(
            metric_readers=[
                PeriodicExportingMetricReader(
                    CloudMonitoringMetricsExporter(
                        project_id=project_id,
                        client=metrics_client  # Pass the monitoring client to the exporter
                    ),
                    export_interval_millis=5000,  # Export interval set to 5 seconds
                )
            ],
            resource=get_aggregated_resources(
                [ProcessResourceDetector(raise_on_error=True)]  # Add process resources
            ),
        )
    )
    print("MeterProvider setup completed.")
    
    meter = metrics.get_meter(__name__)  # Return the configured meter
    print("Meter created successfully.")
    
    return meter, logging_client

# Function to get additional metrics properties (can be extended as needed)
def get_metrics_properties():
    environment = os.getenv("MONITORING_ENV", "staging")
    print(f"Metrics properties - environment: {environment}")
    
    return {
        "environment": environment,
    }

# Function to create OpenTelemetry instruments (counter for IAM errors)
def get_instruments():
    print("Getting instruments (IAM error counter)...")
    
    meter, logging_client = get_meter()
    
    # Create IAM error counter with dynamic name and description
    counter_name = os.getenv("MONITORING_METRICS_IAM_ERROR_COUNTER_NAME", "iam_error_counter")
    counter_description = os.getenv("MONITORING_METRICS_IAM_ERROR_COUNTER_DESCRIPTION", "Tracks IAM errors")
    print(f"Creating IAM error counter: {counter_name}, description: {counter_description}")
    
    iam_error_counter = meter.create_counter(
        name=counter_name,
        description=counter_description,
    )
    
    print("IAM error counter created successfully.")
    
    return iam_error_counter, logging_client

# Function to check for IAM errors in logs and increment the counter
def check_for_iam_errors(iam_error_counter, logging_client):
    logger = logging.getLogger(__name__)
    logger.addHandler(gcp_logging.handlers.CloudLoggingHandler(logging_client))
    logger.info("Checking for IAM errors...")
    print("Checking for IAM errors in logs...")
    
    # Create a filter to fetch logs related to IAM errors for any resource
    filter_str = 'severity="ERROR" AND textPayload:"IAM"'
    print(f"Log filter used: {filter_str}")
    
    # Fetch logs with the filter for IAM errors
    for entry in logging_client.logger("IAM_Error_Logger").list_entries(filter_=filter_str):
        print(f"IAM error detected in log: {entry.payload}")
        
        # Increment the IAM error counter if an error entry is found
        labels = get_metrics_properties()  # Add custom labels
        iam_error_counter.add(1, labels)
        logger.info(f"IAM error detected: {entry.payload}")

# Main loop to check for IAM errors and update metrics periodically
def main():
    print("Starting the main loop...")
    
    # Get the IAM error counter instrument and logging client
    iam_error_counter, logging_client = get_instruments()
    
    # Loop to check for IAM errors every 5 seconds
    while True:
        print("Running IAM error check cycle...")
        check_for_iam_errors(iam_error_counter, logging_client)
        time.sleep(5)
        print("Sleeping for 5 seconds before the next check...")

if __name__ == "__main__":
    main()
