Controlling ingress for your data application in GCP, which involves BigQuery, Datastream, and Dataproc, requires configuring network and security settings to ensure that only authorized traffic can access your resources. Here are different scenarios with solutions:

### Scenario 1: Controlling Ingress to Dataproc Clusters

**Solution:**

1. **Private IP and VPC Peering**:
   - Use private IP addresses for your Dataproc cluster to prevent direct internet access.
   - Set up VPC peering to connect your VPC network with other networks securely.

2. **Firewall Rules**:
   - Create firewall rules to allow ingress traffic only from specific IP ranges or VPCs.
   - Example: Allow traffic from a trusted IP range or another VPC where your Datastream and BigQuery are located.

   ```bash
   gcloud compute firewall-rules create allow-internal-dataproc \
       --network my-vpc \
       --allow tcp,udp,icmp \
       --source-ranges 10.0.0.0/16 \
       --target-tags dataproc-cluster
   ```

3. **IAM Policies**:
   - Use IAM roles and policies to restrict access to Dataproc APIs and resources.
   - Ensure only authorized users or service accounts can create or interact with Dataproc clusters.

### Scenario 2: Securing Ingress to Datastream

**Solution:**

1. **VPC Network Configuration**:
   - Place your Datastream resources in a private subnet within your VPC to limit exposure.
   - Use Private Google Access for your Datastream resources to access Google APIs securely.

2. **Firewall Rules**:
   - Create firewall rules to restrict ingress traffic to Datastream from specific IP ranges or VPC networks.
   - Example: Allow traffic only from the IP range of your application servers or data sources.

   ```bash
   gcloud compute firewall-rules create allow-internal-datastream \
       --network my-vpc \
       --allow tcp:3306 \
       --source-ranges 10.0.1.0/24 \
       --target-tags datastream
   ```

3. **IP Allowlist**:
   - Configure IP allowlists for Datastream to restrict which IP addresses can interact with it.
   - Ensure that only trusted IP addresses or networks are allowed.

### Scenario 3: Controlling Access to BigQuery

**Solution:**

1. **Private Google Access**:
   - Use Private Google Access to enable instances in your VPC to access BigQuery securely without exposing traffic to the internet.

2. **VPC Service Controls**:
   - Implement VPC Service Controls to create a security perimeter around BigQuery and other GCP services.
   - Configure access levels and policies to control which IP ranges or networks can access BigQuery.

   ```bash
   gcloud access-context-manager perimeters create bigquery-perimeter \
       --title="BigQuery Perimeter" \
       --resources=projects/my-project \
       --restricted-services=bigquery.googleapis.com
   ```

3. **IAM Policies**:
   - Use IAM policies to control who can query, view, or manage BigQuery datasets.
   - Assign roles like `bigquery.dataViewer`, `bigquery.dataEditor`, and `bigquery.admin` judiciously.

### Scenario 4: Integrating Dataproc, Datastream, and BigQuery Securely

**Solution:**

1. **Shared VPC**:
   - Use a Shared VPC to host your Dataproc, Datastream, and BigQuery resources in different projects but within the same network.
   - Ensure proper network segmentation and firewall rules are in place.

2. **Service Accounts and IAM**:
   - Create service accounts with minimal required permissions for Dataproc, Datastream, and BigQuery.
   - Use these service accounts to authenticate and authorize interactions between services.

3. **Firewall and Network Tags**:
   - Apply network tags to instances and create firewall rules to allow traffic only between services with specific tags.
   - Example: Allow Dataproc instances to communicate with Datastream and BigQuery through specific firewall rules.

   ```bash
   gcloud compute firewall-rules create allow-dataproc-datastream \
       --network my-vpc \
       --allow tcp:443 \
       --source-tags dataproc \
       --target-tags datastream
   ```

4. **VPC Service Controls**:
   - Extend VPC Service Controls to include all relevant services.
   - Configure access levels to allow only approved ingress and egress traffic within the security perimeter.

   ```bash
   gcloud access-context-manager perimeters update bigquery-perimeter \
       --title="Extended Perimeter" \
       --resources=projects/my-project,projects/another-project \
       --restricted-services=bigquery.googleapis.com,dataproc.googleapis.com,datastream.googleapis.com
   ```

By implementing these solutions, you can effectively control ingress to your GCP data applications, ensuring that only authorized traffic can access your Dataproc clusters, Datastream services, and BigQuery datasets.


In Google Cloud Platform (GCP), internet ingress refers to the incoming network traffic from the internet to your GCP resources. This involves configuring your cloud environment to accept and properly route external requests to the appropriate resources such as virtual machines, cloud storage, or managed services.

Here are key steps and concepts to manage internet ingress in GCP:

1. **VPC Network**:
   - Create a Virtual Private Cloud (VPC) network to host your resources.
   - Define subnets within the VPC network.

2. **Firewall Rules**:
   - Configure firewall rules to allow incoming traffic. By default, all incoming traffic is blocked.
   - Create a firewall rule to allow ingress traffic from the internet on specific ports (e.g., TCP port 80 for HTTP, TCP port 443 for HTTPS).

3. **External IP Addresses**:
   - Assign an external IP address to your VM instances or other resources to make them accessible from the internet.
   - Static or ephemeral external IPs can be used.

4. **Load Balancing**:
   - Use a load balancer to distribute incoming traffic across multiple instances.
   - HTTP(S) Load Balancers, TCP/UDP Load Balancers, and SSL Proxy Load Balancers are common choices.
   - Configure the backend service, URL maps, and health checks.

5. **Cloud CDN**:
   - Enable Cloud CDN to cache and deliver content closer to users, reducing latency.
   - Integrate with HTTP(S) Load Balancer.

6. **Cloud Armor**:
   - Use Cloud Armor to protect your applications from DDoS attacks and other web threats.
   - Create security policies and rules to filter traffic.

7. **Domain and DNS**:
   - Use Cloud DNS to manage your domain names and DNS records.
   - Point your domain to the external IP addresses or load balancers.

### Example Scenario: Setting Up an HTTP(S) Load Balancer

1. **Create a VPC Network**:
   - Navigate to the VPC network section in the GCP Console.
   - Create a new VPC network and define the necessary subnets.

2. **Create VM Instances**:
   - Deploy VM instances in the subnets created.
   - Ensure they have necessary applications/services installed (e.g., a web server).

3. **Set Up Firewall Rules**:
   - Go to the VPC network section and create a new firewall rule.
   - Allow ingress traffic on TCP port 80 and 443.

4. **Reserve an External IP Address**:
   - Reserve a static external IP address for your load balancer.

5. **Configure HTTP(S) Load Balancer**:
   - Create a backend service and add your VM instances.
   - Set up URL maps to route incoming requests to the backend service.
   - Configure health checks to ensure instances are healthy.

6. **Configure Frontend and Obtain IP Address**:
   - Set up the frontend configuration with the reserved external IP.
   - Create an A record in Cloud DNS pointing to the external IP address.

7. **Enable Cloud CDN (Optional)**:
   - Enable Cloud CDN to cache content and improve delivery performance.

8. **Set Up Cloud Armor (Optional)**:
   - Create security policies in Cloud Armor to protect against threats.

By following these steps, you can effectively manage internet ingress to your GCP resources, ensuring they are accessible, scalable, and secure.# Ensure Helm is installed
helm version

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update Helm repositories
helm repo update

# Install Cert Manager CRDs
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.9.1/cert-manager.crds.yaml

# Install Cert Manager using Helm
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespaceeploying a Helm chart using Terraform involves using the Terraform Helm provider. This provider allows you to manage Helm charts using Terraform configuration files. Below are the steps to achieve this:
1.	Install Terraform and Helm: Ensure you have both Terraform and Helm installed on your system.
2.	Configure Terraform provider for Helm: Use the Terraform Helm provider in your Terraform configuration.
3.	Define Kubernetes provider: You also need to configure the Kubernetes provider to interact with your Kubernetes cluster.
Here is an example of how to deploy a Helm chart using Terraform:
Step 1: Install Terraform and Helm
•	Install Terraform: Follow the instructions on the Terraform installation guide.
•	Install Helm: Follow the instructions on the Helm installation guide.
Step 2: Create Terraform configuration files
File: main.tf
provider "kubernetes" {
  config_path = "~/.kube/config"
}

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

resource "helm_release" "my_helm_release" {
  name       = "my-release"
  repository = "https://charts.helm.sh/stable"
  chart      = "nginx"

  set {
    name  = "service.type"
    value = "LoadBalancer"
  }
}
Step 3: Initialize and apply Terraform
1.	Initialize Terraform: Run terraform init to initialize your Terraform configuration.
2.	Apply Terraform configuration: Run terraform apply to deploy the Helm chart.
Step 4: Verify the deployment
•	Check the Helm release: Use helm list to see if the Helm release is deployed.
•	Check the Kubernetes resources: Use kubectl get all to see if the resources created by the Helm chart are running.
Example Breakdown
•	Providers:
•	The kubernetes provider is configured to use the kubeconfig file to interact with the Kubernetes cluster.
•	The helm provider is also configured to use the same kubeconfig file.
•	Helm Release Resource:
•	name: The name of the Helm release.
•	repository: The URL of the Helm chart repository.
•	chart: The name of the chart to deploy.
•	set block: Used to override values in the Helm chart.
Example with Variables and Outputs
To make the configuration more flexible, you can use variables and outputs.
File: variables.tf
variable "kubeconfig_path" {
  description = "Path to the kubeconfig file"
  type        = string
  default     = "~/.kube/config"
}

variable "helm_chart_name" {
  description = "Name of the Helm chart"
  type        = string
  default     = "nginx"
}

variable "helm_chart_repository" {
  description = "Helm chart repository URL"
  type        = string
  default     = "https://charts.helm.sh/stable"
}

variable "helm_release_name" {
  description = "Name of the Helm release"
  type        = string
  default     = "my-release"
}
provider "kubernetes" {
  config_path = var.kubeconfig_path
}

provider "helm" {
  kubernetes {
    config_path = var.kubeconfig_path
  }
}

resource "helm_release" "my_helm_release" {
  name       = var.helm_release_name
  repository = var.helm_chart_repository
  chart      = var.helm_chart_name

  set {
    name  = "service.type"
    value = "LoadBalancer"
  }
}
output "helm_release_name" {
  description = "The name of the Helm release"
  value       = helm_release.my_helm_release.name
}

output "helm_release_status" {
  description = "The status of the Helm release"
  value       = helm_release.my_helm_release.status
}
With this setup, you can easily customize your deployment by changing the values of the variables.
Using the Terraform Helm provider allows you to manage your Helm charts declaratively with Terraform. This approach is useful for integrating Helm with your existing Terraform workflows and managing Kubernetes resources efficiently.

Q. i have vault url where certificate is not there,i want to assign a dns name for my vault url for self signed certificate .i have to deploy cert manager for this .please provide the scnrios and step by step procedure tooo complete the task
To assign a DNS name to your Vault URL and secure it with a self-signed certificate using Cert-Manager, you'll follow these steps:
1.	Install Cert-Manager: Install Cert-Manager in your Kubernetes cluster.
2.	Create a Self-Signed Issuer: Define a self-signed issuer in Kubernetes.
3.	Create a Certificate Resource: Use the self-signed issuer to generate a certificate for your DNS name.
4.	Update Vault with the Certificate: Configure Vault to use the generated certificate.
Here is the detailed step-by-step procedure:
Step 1: Install Cert-Manager
First, install Cert-Manager in your Kubernetes cluster.
Add the Cert-Manager Helm repository:
helm repo add jetstack https://charts.jetstack.io
helm repo update
Install Cert-Manager using Helm:

kubectl create namespace cert-manager
helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1

Step 2: Create a Self-Signed Issuer
Create a self-signed issuer that Cert-Manager can use to issue certificates.
File: selfsigned-issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-issuer
  namespace: cert-manager
spec:
  selfSigned: {}

Apply the issuer configuration:
kubectl apply -f selfsigned-issuer.yaml

Step 3: Create a Certificate Resource
Create a certificate resource that uses the self-signed issuer to generate a certificate for your DNS name.
File: vault-certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: vault-certificate
  namespace: cert-manager
spec:
  dnsNames:
  - vault.example.com   # Replace with your DNS name
  secretName: vault-tls
  issuerRef:
    name: selfsigned-issuer
    kind: Issuer

Apply the certificate configuration:

kubectl apply -f vault-certificate.yaml

Step 4: Update Vault with the Certificate
Retrieve the generated certificate and configure Vault to use it.
Get the certificate and key:
kubectl get secret vault-tls -n cert-manager -o jsonpath='{.data.tls\.crt}' | base64 --decode > tls.crt
kubectl get secret vault-tls -n cert-manager -o jsonpath='{.data.tls\.key}' | base64 --decode > tls.key

Configure Vault
1.	Deploy Vault: If you haven't already deployed Vault, do so with the Helm chart or any other method.
2.	Update Vault Deployment: Modify the Vault deployment to use the generated certificate.
Example Helm values for Vault (values.yaml):
server:
  extraVolumes:
    - name: vault-tls
      secret:
        secretName: vault-tls
  extraVolumeMounts:
    - name: vault-tls
      mountPath: /vault/userconfig/tls/
      readOnly: true
  ha:
    enabled: true
    config: |
      ui = true

      listener "tcp" {
        address     = "0.0.0.0:8200"
        tls_cert_file = "/vault/userconfig/tls/tls.crt"
        tls_key_file  = "/vault/userconfig/tls/tls.key"
      }

      storage "raft" {
        path = "/vault/data"
      }

      api_addr = https://vault.example.com:8200

Deploy or update Vault with these values:
helm upgrade --install vault hashicorp/vault -f values.yaml

Verify the Setup
1.	Check the Certificate: Ensure the certificate is correctly issued and applied by accessing the Vault URL.
2.	DNS Configuration: Ensure that the DNS name (e.g., vault.example.com) points to the Vault service in your Kubernetes cluster.
Summary
By following these steps, you've installed Cert-Manager, created a self-signed issuer, generated a certificate for your Vault DNS name, and configured Vault to use the certificate. This secures your Vault instance with a self-signed certificate.

To implement the setup on an Autopilot GKE cluster, you need to follow these steps:
1.	Create an Autopilot GKE Cluster: Set up your Google Kubernetes Engine (GKE) cluster in Autopilot mode.
2.	Install Cert-Manager: Deploy Cert-Manager in your GKE cluster.
3.	Create a Self-Signed Issuer: Define a self-signed issuer in Kubernetes.
4.	Create a Certificate Resource: Use the self-signed issuer to generate a certificate for your DNS name.
5.	Deploy Vault and Configure It to Use the Certificate: Set up Vault and update it to use the generated certificate.
Step 1: Create an Autopilot GKE Cluster
1.	Install Google Cloud SDK: Ensure that you have the Google Cloud SDK installed. You can download it from here.
2.	Authenticate and Configure GCloud:
gcloud init
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
gcloud config set compute/zone YOUR_COMPUTE_ZONE

Create the GKE Autopilot Cluster:
gcloud container clusters create-auto my-autopilot-cluster --region YOUR_REGION

Get Credentials for the Cluster:
gcloud container clusters get-credentials my-autopilot-cluster --region YOUR_REGION

Step 2: Install Cert-Manager
1.	Add Cert-Manager Helm Repository:
helm repo add jetstack https://charts.jetstack.io
helm repo update
Install Cert-Manager using Helm:

kubectl create namespace cert-manager
helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1


Step 3: Create a Self-Signed Issuer
1.	Create the Self-Signed Issuer Resource:
Create a file named selfsigned-issuer.yaml:
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-issuer
  namespace: cert-manager
spec:
  selfSigned: {}


Apply the Self-Signed Issuer Resource:

kubectl apply -f selfsigned-issuer.yaml

Step 4: Create a Certificate Resource
1.	Create the Certificate Resource:
Create a file named vault-certificate.yaml:
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: vault-certificate
  namespace: cert-manager
spec:
  dnsNames:
  - vault.example.com   # Replace with your DNS name
  secretName: vault-tls
  issuerRef:
    name: selfsigned-issuer
    kind: Issuer


Apply the Certificate Resource:

kubectl apply -f vault-certificate.yaml

Step 5: Deploy Vault and Configure It to Use the Certificate
1.	Deploy Vault: If you haven't already, deploy Vault using Helm.
2.	Retrieve the Generated Certificate:
kubectl get secret vault-tls -n cert-manager -o jsonpath='{.data.tls\.crt}' | base64 --decode > tls.crt
kubectl get secret vault-tls -n cert-manager -o jsonpath='{.data.tls\.key}' | base64 --decode > tls.key

server:
  extraVolumes:
    - name: vault-tls
      secret:
        secretName: vault-tls
  extraVolumeMounts:
    - name: vault-tls
      mountPath: /vault/userconfig/tls/
      readOnly: true
  ha:
    enabled: true
    config: |
      ui = true

      listener "tcp" {
        address     = "0.0.0.0:8200"
        tls_cert_file = "/vault/userconfig/tls/tls.crt"
        tls_key_file  = "/vault/userconfig/tls/tls.key"
      }

      storage "raft" {
        path = "/vault/data"
      }

      api_addr = https://vault.example.com:8200

Deploy or Update Vault with Helm:
helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update
helm upgrade --install vault hashicorp/vault -f values.yaml --namespace default

Step 6: Verify the Setup
1.	Check the Certificate: Ensure the certificate is correctly issued and applied by accessing the Vault URL:
kubectl get certificate vault-certificate -n cert-manager

1.	DNS Configuration: Ensure that the DNS name (e.g., vault.example.com) points to the Vault service in your Kubernetes cluster.
2.	Access Vault: Visit https://vault.example.com:8200 in your browser to verify that the Vault instance is using the certificate.
By following these steps, you will have set up Vault on an Autopilot GKE cluster with a self-signed certificate managed by Cert-Manage




If you already have an Autopilot GKE cluster, you can focus on the steps for deploying Cert-Manager and configuring the Issuer and Certificate using Terraform. Here’s how you can achieve this:
Step 1: Set up Terraform to Use Your Existing GKE Cluster
Ensure you have your GKE cluster credentials set up. You can use gcloud to get the credentials and set up your kubeconfig file.
gcloud container clusters get-credentials YOUR_CLUSTER_NAME --region YOUR_CLUSTER_REGION


Step 2: Create Terraform Configuration for Helm and Cert-Manager
File: main.tf
Step 2: Create Terraform Configuration for Helm and Cert-Manager
File: main.tf
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

provider "kubernetes" {
  config_path = "~/.kube/config"
}

resource "helm_release" "cert_manager" {
  name       = "cert-manager"
  repository = "https://charts.jetstack.io"
  chart      = "cert-manager"
  namespace  = "cert-manager"
  version    = "v1.9.1"

  create_namespace = true

  set {
    name  = "installCRDs"
    value = "true"
  }
}

resource "kubernetes_namespace" "cert_manager" {
  metadata {
    name = "cert-manager"
  }
}

resource "kubernetes_manifest" "selfsigned_issuer" {
  manifest = {
    apiVersion = "cert-manager.io/v1"
    kind       = "Issuer"
    metadata = {
      name      = "selfsigned-issuer"
      namespace = kubernetes_namespace.cert_manager.metadata[0].name
    }
    spec = {
      selfSigned = {}
    }
  }
}

resource "kubernetes_manifest" "vault_certificate" {
  manifest = {
    apiVersion = "cert-manager.io/v1"
    kind       = "Certificate"
    metadata = {
      name      = "vault-certificate"
      namespace = kubernetes_namespace.cert_manager.metadata[0].name
    }
    spec = {
      secretName = "vault-tls"
      issuerRef = {
        name = kubernetes_manifest.selfsigned_issuer.manifest["metadata"]["name"]
        kind = "Issuer"
      }
      dnsNames = ["vault.example.com"] # Replace with your DNS name
    }
  }
}

Step 3: Configure Variables
File: variables.tf
variable "project_id" {
  description = "The GCP project ID"
  type        = string
}

variable "region" {
  description = "The region where the cluster is deployed"
  type        = string
}

variable "cluster_name" {
  description = "The name of the GKE cluster"
  type        = string
}

variable "dns_name" {
  description = "The DNS name for the certificate"
  type        = string
  default     = "vault.example.com" # Replace with your actual DNS name
}
variable "project_id" {
  description = "The GCP project ID"
  type        = string
}

variable "region" {
  description = "The region where the cluster is deployed"
  type        = string
}

variable "cluster_name" {
  description = "The name of the GKE cluster"
  type        = string
}

variable "dns_name" {
  description = "The DNS name for the certificate"
  type        = string
  default     = "vault.example.com" # Replace with your actual DNS name
}

File: terraform.tfvars
project_id   = "your-gcp-project-id"
region       = "your-gcp-region"
cluster_name = "your-cluster-name"
dns_name     = "vault.example.com" # Replace with your actual DNS name

Step 4: Initialize and Apply Terraform Configuration
1.	Initialize Terraform:
terraform init

Apply the Terraform Configuration:
terraform apply


Step 5: Verify the Setup
1.	Check the Cert-Manager Pods:
kubectl get pods -n cert-manager

Check the Issuer:

kubectl get issuer -n cert-manager

Check the Certificate:
kubectl get certificate -n cert-manager

Summary
This Terraform configuration sets up Cert-Manager on your existing Autopilot GKE cluster, creates a self-signed Issuer, and generates a Certificate for your Vault DNS name. Make sure to replace placeholder values (like the DNS name) with your actual values. This approach leverages the Terraform Helm provider to deploy Cert-Manager and the Kubernetes provider to manage Kubernetes resources.





Certainly! Below is the complete Terraform code to deploy Cert-Manager on an existing Autopilot GKE cluster and configure a self-signed Issuer and Certificate. This includes creating the necessary namespace and using Terraform's Helm provider to install Cert-Manager.
Terraform Configuration Files
File: main.tf
provider "google" {
  project = var.project_id
  region  = var.region
}

provider "kubernetes" {
  config_path = "~/.kube/config"
}

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

resource "google_container_cluster" "autopilot_cluster" {
  name     = var.cluster_name
  location = var.region

  autopilot {
    enabled = true
  }
}

resource "google_container_node_pool" "default_pool" {
  name       = "default-pool"
  location   = google_container_cluster.autopilot_cluster.location
  cluster    = google_container_cluster.autopilot_cluster.name
  node_count = 1

  node_config {
    preemptible  = true
    machine_type = "e2-medium"

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform",
    ]
  }

  depends_on = [google_container_cluster.autopilot_cluster]
}

output "kubeconfig" {
  value = google_container_cluster.autopilot_cluster.kube_config_raw
}

resource "helm_release" "cert_manager" {
  name       = "cert-manager"
  repository = "https://charts.jetstack.io"
  chart      = "cert-manager"
  namespace  = kubernetes_namespace.cert_manager.metadata[0].name
  version    = "v1.9.1"

  create_namespace = true

  set {
    name  = "installCRDs"
    value = "true"
  }
}

resource "kubernetes_namespace" "cert_manager" {
  metadata {
    name = "cert-manager"
  }
}

resource "kubernetes_manifest" "selfsigned_issuer" {
  manifest = {
    apiVersion = "cert-manager.io/v1"
    kind       = "Issuer"
    metadata = {
      name      = "selfsigned-issuer"
      namespace = kubernetes_namespace.cert_manager.metadata[0].name
    }
    spec = {
      selfSigned = {}
    }
  }
}

resource "kubernetes_manifest" "vault_certificate" {
  manifest = {
    apiVersion = "cert-manager.io/v1"
    kind       = "Certificate"
    metadata = {
      name      = "vault-certificate"
      namespace = kubernetes_namespace.cert_manager.metadata[0].name
    }
    spec = {
      secretName = "vault-tls"
      issuerRef = {
        name = kubernetes_manifest.selfsigned_issuer.manifest["metadata"]["name"]
        kind = "Issuer"
      }
      dnsNames = [var.dns_name]
    }
  }
}
File: variables.tf
variable "project_id" {
  description = "The GCP project ID"
  type        = string
}

variable "region" {
  description = "The region where the cluster is deployed"
  type        = string
}

variable "cluster_name" {
  description = "The name of the GKE cluster"
  type        = string
}

variable "dns_name" {
  description = "The DNS name for the certificate"
  type        = string
  default     = "vault.example.com" # Replace with your actual DNS name
}


File: terraform.tfvars
project_id   = "your-gcp-project-id"
region       = "your-gcp-region"
cluster_name = "your-cluster-name"
dns_name     = "vault.example.com" # Replace with your actual DNS name


Step-by-Step Guide
1.	Get GKE Cluster Credentials:
Before running Terraform, you need to authenticate your local kubectl configuration to your GKE cluster.
gcloud container clusters get-credentials YOUR_CLUSTER_NAME --region YOUR_CLUSTER_REGION

1.	Replace YOUR_CLUSTER_NAME and YOUR_CLUSTER_REGION with your actual cluster name and region.
2.	Initialize and Apply Terraform Configuration:
•	Initialize Terraform
terraform init


Apply the Terraform Configuration:
terraform apply


Verify the Setup:
•	Check the Cert-Manager Namespace:
kubectl get namespace cert-manager

•	Ensure the cert-manager namespace is created.
•	Check the Cert-Manager Pods:
kubectl get pods -n cert-manager
To create a production-ready Terraform configuration for setting up centralized logging on Google Cloud Platform (GCP) with a user-managed service account, you'll need to:

1. Create a log bucket in the central project.
2. Create a user-managed service account with appropriate permissions.
3. Set up log sinks in the source projects to route logs to the central log bucket.
4. Use the service account in the log sinks.

Here is the Terraform code to achieve this:

### Step 1: Create Log Bucket in Central Project

```hcl
provider "google" {
  project = "central-project-id"
  region  = "us-central1"
}

resource "google_logging_project_bucket_config" "log_bucket" {
  bucket_id = "central-log-bucket"
  location  = "global"
  retention_days = 30
}
```

### Step 2: Create User-Managed Service Account

```hcl
resource "google_service_account" "logging_sa" {
  account_id   = "logging-sink"
  display_name = "Logging Sink Service Account"
}

resource "google_project_iam_member" "log_writer" {
  project = "central-project-id"
  role    = "roles/logging.bucketWriter"
  member  = "serviceAccount:${google_service_account.logging_sa.email}"
}
```

### Step 3: Configure Log Sink in Source Projects

```hcl
provider "google" {
  project = "source-project-id"
  region  = "us-central1"
}

resource "google_logging_project_sink" "log_sink" {
  name        = "central-log-sink"
  destination = "logging.googleapis.com/projects/central-project-id/locations/global/buckets/central-log-bucket"
  filter      = "logName:projects/source-project-id/logs/"
  writer_identity = "serviceAccount:${google_service_account.logging_sa.email}"
}

resource "google_project_iam_member" "sink_writer" {
  project = "source-project-id"
  role    = "roles/logging.logWriter"
  member  = "serviceAccount:${google_logging_project_sink.log_sink.writer_identity}"
}
```

### Step 4: Grant Service Account Permissions to Write Logs

```hcl
resource "google_project_iam_member" "central_log_bucket_writer" {
  project = "central-project-id"
  role    = "roles/logging.bucketWriter"
  member  = "serviceAccount:${google_service_account.logging_sa.email}"
}
```

### Full Terraform Configuration

Combine all the steps above into a single Terraform configuration. Here is how it looks:

```hcl
# Define providers for both central and source projects
provider "google" {
  project = "central-project-id"
  region  = "us-central1"
  alias   = "central"
}

provider "google" {
  project = "source-project-id"
  region  = "us-central1"
  alias   = "source"
}

# Create Log Bucket in Central Project
resource "google_logging_project_bucket_config" "log_bucket" {
  provider    = google.central
  bucket_id   = "central-log-bucket"
  location    = "global"
  retention_days = 30
}

# Create User-Managed Service Account
resource "google_service_account" "logging_sa" {
  provider    = google.central
  account_id   = "logging-sink"
  display_name = "Logging Sink Service Account"
}

# Grant Service Account Permission to Write to Log Bucket
resource "google_project_iam_member" "central_log_bucket_writer" {
  provider = google.central
  project = "central-project-id"
  role    = "roles/logging.bucketWriter"
  member  = "serviceAccount:${google_service_account.logging_sa.email}"
}

# Configure Log Sink in Source Project
resource "google_logging_project_sink" "log_sink" {
  provider    = google.source
  name        = "central-log-sink"
  destination = "logging.googleapis.com/projects/central-project-id/locations/global/buckets/central-log-bucket"
  filter      = "logName:projects/source-project-id/logs/"
  writer_identity = "serviceAccount:${google_service_account.logging_sa.email}"
}

# Grant Log Sink Service Account Permission to Write Logs in Source Project
resource "google_project_iam_member" "sink_writer" {
  provider = google.source
  project = "source-project-id"
  role    = "roles/logging.logWriter"
  member  = "serviceAccount:${google_logging_project_sink.log_sink.writer_identity}"
}
```

### Explanation

- **Providers**: Defined for both central and source projects to manage resources in multiple projects.
- **Log Bucket**: Created in the central project with a defined retention period.
- **Service Account**: User-managed service account with `roles/logging.bucketWriter` role in the central project.
- **Log Sink**: Created in the source project with the destination set to the log bucket in the central project.
- **IAM Roles**: Ensured the service account has the necessary permissions to write logs.

This configuration is scalable and can be used to add more source projects by replicating the source project configurations.

•	You should see the Cert-Manager pods running.
•	Check the Issuer:
kubectl get issuer -n cert-manager

•	You should see the selfsigned-issuer.
•	Check the Certificate:
kubectl get certificate -n cert-manager

kubectl get certificate -n cert-manager kubectl get certificate -n cert-manager

1.	
•	You should see the vault-certificate and its status should indicate it is ready.
Summary
This Terraform configuration sets up Cert-Manager on your existing Autopilot GKE cluster, creates a namespace for Cert-Manager, and configures the Issuer and Certificate resources within that namespace. This setup leverages Terraform for consistent and reproducible infrastructure management. Make sure to replace placeholder values (like the DNS name) with your actual values.
If you don't have a DNS name but have a URL with an IP address and port to access Vault, you can still configure Cert-Manager to issue a certificate for that URL. Here's how to modify the Terraform setup to issue a certificate for the Vault service using the IP address instead of a DNS name. ### Step-by-Step Guide for Issuing a Certificate for an IP Address 1. **Modify Terraform Configuration**: Update the Terraform configuration to issue a certificate for the Vault service's IP address. #### File: `main.tf` ```hcl provider "google" { project = var.project_id region = var.region } provider "kubernetes" { config_path = "~/.kube/config" } provider "helm" { kubernetes { config_path = "~/.kube/config" } } resource "google_container_cluster" "autopilot_cluster" { name = var.cluster_name location = var.region autopilot { enabled = true } } resource "google_container_node_pool" "default_pool" { name = "default-pool" location = google_container_cluster.autopilot_cluster.location cluster = google_container_cluster.autopilot_cluster.name node_count = 1 node_config { preemptible = true machine_type = "e2-medium" oauth_scopes = [ "https://www.googleapis.com/auth/cloud-platform", ] } depends_on = [google_container_cluster.autopilot_cluster] } output "kubeconfig" { value = google_container_cluster.autopilot_cluster.kube_config_raw } resource "helm_release" "cert_manager" { name = "cert-manager" repository = "https://charts.jetstack.io" chart = "cert-manager" namespace = kubernetes_namespace.cert_manager.metadata[0].name version = "v1.9.1" create_namespace = true set { name = "installCRDs" value = "true" } } resource "kubernetes_namespace" "cert_manager" { metadata { name = "cert-manager" } } resource "kubernetes_manifest" "selfsigned_issuer" { manifest = { apiVersion = "cert-manager.io/v1" kind = "Issuer" metadata = { name = "selfsigned-issuer" namespace = kubernetes_namespace.cert_manager.metadata[0].name } spec = { selfSigned = {} } } } resource "kubernetes_manifest" "vault_certificate" { manifest = { apiVersion = "cert-manager.io/v1" kind = "Certificate" metadata = { name = "vault-certificate" namespace = kubernetes_namespace.cert_manager.metadata[0].name } spec = { secretName = "vault-tls" issuerRef = { name = kubernetes_manifest.selfsigned_issuer.manifest["metadata"]["name"] kind = "Issuer" } ipAddresses = [var.vault_ip] # Using IP address instead of DNS name } } } resource "kubernetes_namespace" "vault" { metadata { name = "vault" } } resource "kubernetes_service" "vault" { metadata { name = "vault" namespace = kubernetes_namespace.vault.metadata[0].name } spec { selector = { app = "vault" } ports { port = 8200 target_port = 8200 } } } resource "kubernetes_ingress" "vault" { metadata { name = "vault-ingress" namespace = kubernetes_namespace.vault.metadata[0].name annotations = { "kubernetes.io/ingress.class" = "nginx" "cert-manager.io/issuer" = "selfsigned-issuer" "nginx.ingress.kubernetes.io/ssl-redirect" = "true" } } spec { tls { hosts = [var.vault_ip] secret_name = "vault-tls" } rule { host = var.vault_ip http { path { path = "/" backend { service_name = kubernetes_service.vault.metadata[0].name service_port = "8200" } } } } } } ``` #### File: `variables.tf` ```hcl variable "project_id" { description = "The GCP project ID" type = string } variable "region" { description = "The region where the cluster is deployed" type = string } variable "cluster_name" { description = "The name of the GKE cluster" type = string } variable "vault_ip" { description = "The IP address for the Vault service" type = string } ``` #### File: `terraform.tfvars` ```hcl project_id = "your-gcp-project-id" region = "your-gcp-region" cluster_name = "your-cluster-name" vault_ip = "your-vault-ip" # Replace with your actual Vault service IP ``` ### Step-by-Step Guide 1. **Authenticate and Set Context for GKE Cluster**: Before running Terraform, you need to authenticate your local `kubectl` configuration to your GKE cluster. ```bash gcloud container clusters get-credentials YOUR_CLUSTER_NAME --region YOUR_CLUSTER_REGION ``` Replace `YOUR_CLUSTER_NAME` and `YOUR_CLUSTER_REGION` with your actual cluster name and region. 2. **Create Terraform Configuration Files**: Ensure you have the above Terraform files (`main.tf`, `variables.tf`, `terraform.tfvars`) in your working directory. 3. **Initialize and Apply Terraform Configuration**: - **Initialize Terraform**: ```bash terraform init ``` - **Apply the Terraform Configuration**: ```bash terraform apply ``` Follow the prompts and confirm the apply action. ### Verify the Setup 1. **Check the Cert-Manager Namespace**: ```bash kubectl get namespace cert-manager ``` Ensure the `cert-manager` namespace is created. 2. **Check the Cert-Manager Pods**: ```bash kubectl get pods -n cert-manager ``` You should see the Cert-Manager pods running. 3. **Check the Issuer**: ```bash kubectl get issuer -n cert-manager ``` You should see the `selfsigned-issuer`. 4. **Check the Certificate**: ```bash kubectl get certificate -n cert-manager ``` You should see the `vault-certificate` and its status should indicate it is ready. 5. **Check the Ingress**: ```bash kubectl get ingress -n vault ``` Ensure the Ingress is correctly configured with the IP address. ### Summary This documentation covers the entire process of setting up Cert-Manager on an existing Autopilot GKE cluster using Terraform, creating a namespace, setting up a self-signed Issuer and Certificate, and mapping the Vault URL with the IP address using an Ingress resource. The setup leverages Terraform for consistent and reproducible infrastructure management. ### Troubleshooting - **Authentication Issues**: Ensure your `gcloud` SDK is authenticated with the correct project and region. - **Namespace Not Found**: If the namespace is not created, ensure that the `create_namespace` attribute is set to `true` in the Helm release resource. - **Pod Errors**: Check the logs of Cert-Manager pods using `kubectl logs` to diagnose any issues with the deployment. - **Ingress Issues**: Ensure the Ingress controller (e.g., NGINX Ingress) is
